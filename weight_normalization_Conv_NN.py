# -*- coding: utf-8 -*-
"""weight_normalization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EayRODXu_WoUFw8SsRACI5D5mMkq8Nyq
"""

import torch 
import torchvision
import torch.nn as nn
from torch.nn.utils import weight_norm
from torch import _weight_norm
import torch.nn.functional as F
from torch.nn.parameter import Parameter, UninitializedParameter
import torch.optim as optim
from torchvision import datasets, transforms

import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE

transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),
                    torchvision.transforms.Normalize((0.1307,),(0.3081,))])

batch_size_train, batch_size_test = 64, 1000
'''
train_dataset = torchvision.datasets.MNIST('/data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.MNIST('/data', train=False, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)
'''
batch_size_train, batch_size_test = 64, 1000
transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

train_dataset = torchvision.datasets.CIFAR10('/data', train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10('/data', train=False, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size_test, shuffle=False)

"""$\mathbf{w}$ = $\dfrac{g}{\|\mathbf{v}\|} \mathbf{v}$

Let us take a step back and evaluate what needs to be done in order to recompute the weight parameterization for our model.
   We know that we can access all the parameters of a PyTorch tensor as --> for name, param in model.named_parameters():
   knowing that we can gain access to these parameter names and values, we can then add, remove, or modify the parameters directly. 
   Under normal parameterization protocol, the weight vector that is associated with each neuron has both a direction and a magnitude. 
   
   This approach will decouple the two separate termed parameters -> and return a weighted vector v and a scalar parameter g to perform 
   stochastic gradient descent with respect to those parameters instead.
"""

#this function takes in the weight attribute 'v' which is just the direction of the tensor and the magnitude
#in order to decouple the two, we must take the contiguous norm along dimension=1 of the same corresponding size input length v.
'''this is taken from the CPP implementation @ https://github.com/pytorch/pytorch/blob/517c7c98610402e2746586c78987c64c28e024aa/aten/src/ATen/native/WeightNorm.cpp#L19'''
#this must be catered to python implementation as seen below

def norm_except_dim(v, pow, dim):
    if dim == -1:
      return v.norm(pow);
    if dim != 0:
        v = v.transpose(0, dim)

    output_size = (v.size(0),) + (1,) * (v.dim() - 1)

    v = v.contiguous().view(v.size(0), -1).norm(dim=1).view(*output_size) #<- unpacks the output_size into an iterable (just one dimension)
    if dim != 0:
        v = v.transpose(0, dim)
    return v

'''This github open forum for manual implementation necessities for the WeightNorm class is very helpful. Class needs a __call__ function.
    weight_norm tries to use an instance of a class with __call__ defined (WeightNorm) as a forward pre hook. 

    OF TYPE fn = WeightNorm(name, dim)

    module.register_forward_pre_hook(fn)

    class WeightNorm:
        def __call__(...):
            ...
   https://github.com/pytorch/pytorch/issues/57289'''

def calc_weight_norm(v, g, dim):
  return torch.multiply(v, torch.multiply(g, torch.norm(v,2)))

  '''Must have explicit function declarations, similar to the forward call of a super(nn.Module) class. This includes
     class WeightNorm(), @staticmethod apply(), __call__(), remove(), weight_norm(), remove_weight_norm().'''
     #The latter two are necessities for the prehook that is called before each forward() function call in the NN class

class WeightNorm():
  def __init__(self, name="weight", dim:int = 0):
    self.name = name
    self.dim = dim

  '''Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. 
     This replaces the parameter specified by name (e.g. 'weight') with two parameters: one specifying the magnitude (e.g. 'weight_g') 
     and one specifying the direction (e.g. 'weight_v'). Weight normalization is implemented via a hook that recomputes the weight tensor 
     from the magnitude and direction before every forward() call.
     
    return v*(g/at::norm_except_dim(v, 2, dim));'''

  def compute_weight(self, module: nn.Module):
    g = getattr(module, self.name + '_g')
    v = getattr(module, self.name + '_v')
    return calc_weight_norm(v, g, self.dim)
    #return _weight_norm(v, g, self.dim)
    
  @staticmethod
  def apply(module, name: str, dim: int) -> 'WeightNorm':
    for k, hook in module._forward_pre_hooks.items():
        if isinstance(hook, WeightNorm) and hook.name == name:
            raise RuntimeError("Cannot register two weight_norm hooks on the same parameter {}".format(name))
    if dim is None:
        dim = -1

    fn = WeightNorm(name, dim)

    weight = getattr(module, name)

    if isinstance(weight, UninitializedParameter):
        raise ValueError('The module passed to `WeightNorm` can\'t have uninitialized parameters. '
            'Make sure to run the dummy forward before applying weight normalization')
    # remove w from parameter list
    del module._parameters[name]

    # add g and v as new parameters and express w as g/||v|| * v
    module.register_parameter(name + '_g', Parameter(norm_except_dim(weight, 2, dim).data))
    module.register_parameter(name + '_v', Parameter(weight.data))
    setattr(module, name, fn.compute_weight(module))
    #print(f'The weight is: {weight}')
    #once we retrieve the weight value from above, we can delete the model parameter and then insert new weight_g and weight_v to the module
    setattr(module, name, fn.compute_weight(module))

    #utilize forward prehook to execute before each forward pass of the model
    #Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call.

    module.register_forward_pre_hook(fn)
    return fn

  def remove(self, module) -> None:
    weight = self.compute_weight(module)
    delattr(module, self.name)
    del module._parameters[self.name + '_g']
    del module._parameters[self.name + '_v']
    setattr(module, self.name, Parameter(weight.data))

  def __call__(self, module, inputs):
    setattr(module, self.name, self.compute_weight(module))

#def weight_norm(module: nn.Module, name: str = 'weight', dim: int = 0) -> nn.Module:
#  WeightNorm.apply(module, name, dim)
#  return module
def remove_weight_norm(module: nn.Module, name: str = 'weight') -> nn.Module:
  for k, hook in module._forward_pre_hooks.items():
    if isinstance(hook, WeightNorm) and hook.name == name:
      hook.remove(module)
      del module._forward_pre_hooks[k]
      return module
  raise ValueError("weight_norm of '{}' not found in {}".format(name, module))

class weightCNN(nn.Module):
  def __init__(self):
    super(weightCNN, self).__init__()
    self.conv1 = weight_norm(nn.Conv2d(3, 16, 5, 1))
    self.conv2 = nn.Conv2d(16, 48, 5, 1)
    self.fc1 = weight_norm(nn.Linear(5*5*48, 512))
    self.fc2 = nn.Linear(512, 10)
    #self.fc3 = nn.Linear(256, 10)

  def forward(self, x):
      """
      X -> (N, C, W, H) -> (batchsize, channel (grey = 1, color = rgb = 3), width, height)
      """
      x = F.relu(self.conv1(x))
      x = F.max_pool2d(x, 2, 2)
      x = F.relu(self.conv2(x))
      x = F.max_pool2d(x, 2, 2)
      x = torch.flatten(x, 1)
      x = F.relu(self.fc1(x))
      x = self.fc2(x)
      return F.log_softmax(x, dim=1)
 
class no_weightCNN(nn.Module):
  def __init__(self):
    super(no_weightCNN, self).__init__()
    # convolutional layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
    self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
    # max pooling layer
    self.pool = nn.MaxPool2d(2, 2)
    # fully connected layers
    self.fc1 = nn.Linear(64 * 4 * 4, 512)
    self.fc2 = nn.Linear(512, 64)
    self.fc3 = nn.Linear(64, 10)
    # dropout
    self.dropout = nn.Dropout(p=.5)

  def forward(self, x):
    # convolutional -> max pooling 
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = self.pool(F.relu(self.conv3(x)))
    x = torch.flatten(x, 1)
    # fully connected layers
    x = self.dropout(F.relu(self.fc1(x)))
    x = self.dropout(F.relu(self.fc2(x)))
    x = self.fc3(x)
    return F.log_softmax(x, dim=1)

class my_weightCNN(nn.Module):
  def __init__(self):
    super(my_weightCNN, self).__init__()
    # convolutional layer
    self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
    self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
    self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
    # max pooling layer
    self.pool = nn.MaxPool2d(2, 2)
    # fully connected layers
    self.fc1 = nn.Linear(64 * 4 * 4, 512)
    self.fc2 = nn.Linear(512, 64)
    self.fc3 = nn.Linear(64, 10)
    # dropout
    self.dropout = nn.Dropout(p=.5)

  def forward(self, x):
    # convolutional -> max pooling 
    x = self.pool(F.relu(self.conv1(x)))
    x = self.pool(F.relu(self.conv2(x)))
    x = self.pool(F.relu(self.conv3(x)))
    x = torch.flatten(x, 1)
    # fully connected layers
    x = self.dropout(F.relu(self.fc1(x)))
    x = self.dropout(F.relu(self.fc2(x)))
    x = self.fc3(x)
    return F.log_softmax(x, dim=1)

train_losses = []
train_counter = []
test_losses = []
test_counter = []

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train(model, epoch, train_loader, optimizer):
  return_loss = 0
  model.train() # we need to set the mode for our model

  for batch_idx, (images, targets) in enumerate(train_loader):
    images, targets = images.to(device), targets.to(device)
    optimizer.zero_grad()
    output = model(images)
    loss = F.nll_loss(output, targets) # Here is a typical loss function (negative log likelihood)
    loss.backward()
    optimizer.step()

    if batch_idx % 10 == 0: # We record our output every 10 batches
      train_losses.append(loss.item()) # item() is to get the value of the tensor directly
      train_counter.append(
        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))
    if batch_idx % 100 == 0: # We visulize our output every 10 batches
      print(f'Epoch {epoch}: [{batch_idx*len(images)}/{len(train_loader.dataset)}] Loss: {loss.item()}')

def test(model, epoch, test_loader):

  model.eval() # we need to set the mode for our model

  test_loss = 0
  correct = 0

  with torch.no_grad():
    for images, targets in test_loader:
      images, targets = images.to(device), targets.to(device)
      output = model(images)
      test_loss += F.nll_loss(output, targets, reduction='sum').item()
      pred = output.data.max(1, keepdim=True)[1] # we get the estimate of our result by look at the largest class value
      correct += pred.eq(targets.data.view_as(pred)).sum()#.item # sum up the corrected samples
  
  test_loss /= len(test_loader.dataset)
  test_losses.append(test_loss)
  test_counter.append(len(train_loader.dataset)*epoch)
  
  print(f'Test result on epoch {epoch}: Avg loss is {test_loss}, Accuracy: {100.*correct/len(test_loader.dataset)}%')
  return test_loss

loss_vals_reg_weight = []
loss_vals_no_weight = []
loss_vals_my_weight = []
epoch_list = np.linspace(1,10,10)
def main1():
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')

  model = weightCNN().to(device)
  model_none = no_weightCNN().to(device)
  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)
  max_epoch = 10
  
  for epoch in range(1, max_epoch+1):
    train(model, epoch, train_loader, optimizer)
    loss = test(model, epoch, test_loader)
    loss_vals_reg_weight.append(loss)

  
  optimizer = optim.SGD(model_none.parameters(), lr=0.01, momentum=0.8)
  for epoch in range(1, max_epoch+1):
    train(model_none, epoch, train_loader, optimizer)
    loss_vals_no_weight.append(test(model_none, epoch, test_loader))

  torch.save(model.state_dict(),"ece570_weight_norm.pth")
  plt.plot(epoch_list, loss_vals_no_weight, label = "Non Weighted Normalization", linestyle="-", color='b')
  plt.plot(epoch_list, loss_vals_reg_weight, label = "Self Implemented Weighted Normalization", linestyle="-", color='r')
  plt.xlabel("Epoch")
  plt.ylabel("Resulting Test Loss")
  plt.legend()
  plt.show()

if __name__ == '__main__':
  main1()

